{
 "cells": [
  {
   "cell_type": "code",
   "id": "3c8eabd4-aaf6-4329-8685-ae0a1af6ab5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:15.255066Z",
     "start_time": "2024-11-10T10:52:14.763622Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4ee06c5a-347e-4ab9-9fe3-020531e9f64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:16.234681Z",
     "start_time": "2024-11-10T10:52:16.227972Z"
    }
   },
   "source": [
    "# Load the credentials\n",
    "def load_credentials(file_path):\n",
    "    credentials = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespaces\n",
    "            # Skip empty lines or lines that don't contain an \"=\"\n",
    "            if not line or \"=\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\"=\", 1)  # Split only at the first \"=\"\n",
    "            credentials[key] = value\n",
    "    return credentials\n",
    "\n",
    "credentials_file = \".env\"  # Update with the correct file path\n",
    "credentials = load_credentials(credentials_file)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "244782b6-9ffd-4c88-833c-c4dbc2e49145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:17.311550Z",
     "start_time": "2024-11-10T10:52:17.308514Z"
    }
   },
   "source": [
    "token = credentials[\"TOKEN\"]\n",
    "org = credentials[\"ORG\"]\n",
    "bucket = credentials[\"BUCKET\"]\n",
    "url = credentials[\"URL\"]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0bb46707-474c-40f4-9d16-5b18133577c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:17.698939Z",
     "start_time": "2024-11-10T10:52:17.689818Z"
    }
   },
   "source": [
    "# Initialize a client instance\n",
    "client = InfluxDBClient(\n",
    "    url=url,\n",
    "    token=token,\n",
    "    org=org,\n",
    "    timeout=30000\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:19.061715Z",
     "start_time": "2024-11-10T10:52:19.057227Z"
    }
   },
   "cell_type": "code",
   "source": "station_ids = [\"034062022\", \"035062022\", \"084072023\"]",
   "id": "820129355d874710",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:19.652484Z",
     "start_time": "2024-11-10T10:52:19.648847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_query(station_id, field, agg_fn, yield_name):\n",
    "    return f'''\n",
    "        from(bucket: \"oxus2\")\n",
    "        |> range(start: -inf)\n",
    "        |> filter(fn: (r) => r[\"stationID\"] == \"{station_id}\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] == \"{field}\")\n",
    "        |> aggregateWindow(every: 1d, fn: {agg_fn}, createEmpty: false)\n",
    "        |> yield(name: \"{yield_name}\")\n",
    "    '''\n",
    "\n",
    "# Define the aggregation functions, fields, and corresponding yield names\n",
    "agg_fns = [\n",
    "    (\"AirT\", \"min\", \"daily_min_temp\"),\n",
    "    (\"AirT\", \"max\", \"daily_max_temp\"),\n",
    "    (\"AirH\", \"mean\", \"daily_avg_hum\"),\n",
    "    (\"AirT\", \"mean\", \"daily_avg_temp\")\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the queries\n",
    "station_queries = {}\n",
    "\n",
    "# Loop through each station_id and aggregation function\n",
    "for station_id in station_ids:\n",
    "    queries = []\n",
    "    for field, agg_fn, yield_name in agg_fns:\n",
    "        query = format_query(station_id, field, agg_fn, yield_name)\n",
    "        queries.append(query)\n",
    "    station_queries[station_id] = queries"
   ],
   "id": "b499d78c8c00673",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:31.428176Z",
     "start_time": "2024-11-10T10:52:24.556349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize an empty dictionary to store the combined DataFrames for each station\n",
    "station_data = {}\n",
    "\n",
    "# Loop through each station_id and aggregation function\n",
    "for station_id in station_ids:\n",
    "    # Initialize an empty list to store data for each query\n",
    "    all_query_data = []\n",
    "    \n",
    "    for field, agg_fn, yield_name in agg_fns:\n",
    "        # Format the query for this station and aggregation\n",
    "        query = format_query(station_id, field, agg_fn, yield_name)\n",
    "        \n",
    "        # Execute the query\n",
    "        tables = client.query_api().query(query)\n",
    "        \n",
    "        # Parse the query results into a list of dictionaries\n",
    "        query_results = []\n",
    "        for table in tables:\n",
    "            for record in table.records:\n",
    "                query_results.append({\n",
    "                    \"time\": record.get_time(),\n",
    "                    yield_name: record.get_value()\n",
    "                })\n",
    "        \n",
    "        # Convert query results to a DataFrame\n",
    "        df = pd.DataFrame(query_results).set_index(\"time\")\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        df = df.round(2)\n",
    "        all_query_data.append(df)\n",
    "    \n",
    "    # Concatenate all the DataFrames for this station into one table\n",
    "    combined_df = pd.concat(all_query_data, axis=1)\n",
    "    combined_df['week'] = ((combined_df.index.dayofyear - 1) // 7 + 1).round(0)\n",
    "    combined_df.insert(0, 'week', combined_df.pop('week'))\n",
    "    \n",
    "    # Store the combined DataFrame for this station\n",
    "    station_data[station_id] = combined_df"
   ],
   "id": "614bbaffa2fc506",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "74ac9a28-d093-476e-a149-dd8753becc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:32.398741Z",
     "start_time": "2024-11-10T10:52:32.395802Z"
    }
   },
   "source": [
    "db_user = credentials['USER']\n",
    "db_password = credentials['PASSWORD']\n",
    "db_host = credentials['HOST']\n",
    "db_port = credentials['PORT']\n",
    "db_name = credentials['DATABASE']"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "126aa5fe-7072-4618-bcb1-d1671301d877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T10:52:53.496127Z",
     "start_time": "2024-11-10T10:52:34.599393Z"
    }
   },
   "source": [
    "engine = create_engine(\n",
    "    f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    ")\n",
    "\n",
    "for station_id in station_ids:\n",
    "    station_data[station_id].to_sql(f'{station_id}', engine, if_exists='replace')\n",
    "\n",
    "print(\"Data is saved successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is saved successfully!\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
